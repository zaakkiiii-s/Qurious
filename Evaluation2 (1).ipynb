{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95MEKJr5u-UN"
      },
      "outputs": [],
      "source": [
        "#PIP INSTALLS:\n",
        "!pip install wikipedia keybert sentence-transformers transformers spacy datasets evaluate matplotlib\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS:\n",
        "import wikipedia\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "import re\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "from google.colab import drive, files\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "xcnJBsLdvTYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINITIONS:\n",
        "\n",
        "# Common slang dictionary\n",
        "SLANG_DICT = {\n",
        "    \"u\": \"you\", \"ur\": \"your\", \"r\": \"are\", \"y\": \"why\",\n",
        "    \"wat\": \"what\", \"wats\": \"what is\", \"wanna\": \"want to\",\n",
        "    \"gonna\": \"going to\", \"tho\": \"though\", \"pls\": \"please\",\n",
        "    \"thx\": \"thanks\", \"lol\": \"\", \"omg\": \"oh my god\",\n",
        "    \"idk\": \"I don't know\", \"btw\": \"by the way\", \"smh\": \"shaking my head\",\n",
        "    \"bff\": \"best friend forever\", \"brb\": \"be right back\", \"tbh\": \"to be honest\",\n",
        "    \"fyi\": \"for your information\", \"lmao\": \"laughing my ass off\", \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"ttyl\": \"talk to you later\", \"np\": \"no problem\", \"yolo\": \"you only live once\",\n",
        "    \"fam\": \"family\", \"lit\": \"exciting or excellent\", \"savage\": \"ruthless or harsh\",\n",
        "    \"slay\": \"to do something exceptionally well\", \"bae\": \"before anyone else\", \"cray\": \"crazy\",\n",
        "    \"fomo\": \"fear of missing out\", \"swag\": \"style or coolness\", \"vibe\": \"mood or atmosphere\",\n",
        "    \"tbh\": \"to be honest\", \"imo\": \"in my opinion\", \"srsly\": \"seriously\", \"jk\": \"just kidding\",\n",
        "    \"ppl\": \"people\", \"wyd\": \"what you doing\",\n",
        "    \"wth\": \"what the hell\",\n",
        "    \"ik\": \"I know\", \"ikr\": \"I know, right?\", \"lml\": \"laughing madly\", \"hmu\": \"hit me up\",\n",
        "    \"mfw\": \"my face when\",\n",
        "}\n",
        "\n",
        "def extract_keyword_spacy_keybert(question):\n",
        "    doc = nlp(question)\n",
        "    question_lower = question.lower()\n",
        "\n",
        "    priority_entity_labels = [\"PERSON\", \"ORG\", \"GPE\", \"FAC\", \"LOC\"]\n",
        "    named_entities = [ent.text for ent in doc.ents if ent.label_ in priority_entity_labels]\n",
        "\n",
        "    if named_entities:\n",
        "        return named_entities[0]\n",
        "\n",
        "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "\n",
        "    keywords = kw_model.extract_keywords(question, top_n=3, stop_words='english')\n",
        "    bert_keywords = [kw[0] for kw in keywords]\n",
        "\n",
        "\n",
        "    candidates = list(dict.fromkeys(noun_phrases + bert_keywords))\n",
        "\n",
        "    filtered_candidates = [\n",
        "        c for c in candidates\n",
        "        if c.lower() not in STOP_WORDS and len(c.strip()) > 2]\n",
        "\n",
        "\n",
        "    filtered_candidates = sorted(filtered_candidates, key=lambda x: question_lower.find(x.lower()))\n",
        "\n",
        "    return filtered_candidates[0] if filtered_candidates else question\n",
        "\n",
        "def get_wikipedia_article(query):\n",
        "    try:\n",
        "\n",
        "        try:\n",
        "            page = wikipedia.page(query, auto_suggest=False)\n",
        "            print(f\"Retrieved (exact match): {page.title}\")\n",
        "            return page.content\n",
        "        except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
        "            pass\n",
        "\n",
        "\n",
        "        search_results = wikipedia.search(query)\n",
        "        if not search_results:\n",
        "            print(\"No search results found.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        for title in search_results:\n",
        "            try:\n",
        "                page = wikipedia.page(title)\n",
        "                if \"replica\" not in page.title.lower():\n",
        "                    print(f\"Retrieved: {page.title}\")\n",
        "                    return page.content\n",
        "            except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
        "                continue\n",
        "\n",
        "        print(\"No valid Wikipedia page found.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_best_wikipedia_article(question, top_n=5):\n",
        "    search_results = wikipedia.search(question, results=top_n)\n",
        "    print(\"Candidates:\", search_results)\n",
        "    summaries = []\n",
        "    titles = []\n",
        "    for title in search_results:\n",
        "        try:\n",
        "            summary = wikipedia.summary(title, sentences=3)\n",
        "            summaries.append(summary)\n",
        "            titles.append(title)\n",
        "        except wikipedia.exceptions.DisambiguationError:\n",
        "            continue\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            continue\n",
        "\n",
        "    if not summaries:\n",
        "        return None\n",
        "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "    summary_embeddings = embedder.encode(summaries, convert_to_tensor=True)\n",
        "    cosine_scores = util.cos_sim(question_embedding, summary_embeddings)\n",
        "\n",
        "    best_idx = cosine_scores.argmax().item()\n",
        "    best_title = titles[best_idx]\n",
        "    best_page = wikipedia.page(best_title)\n",
        "\n",
        "    print(f\"Best match: {best_title}\")\n",
        "    return best_page.content\n",
        "\n",
        "def expand_question(question):\n",
        "    \"\"\"Replace common slangs and shorthand in questions.\"\"\"\n",
        "    words = question.lower().split()\n",
        "    expanded = [SLANG_DICT.get(w, w) for w in words]\n",
        "    return ' '.join(expanded)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove unwanted characters and normalize spacing.\"\"\"\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove references like [1]\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def chunk_text_by_tokens(text, max_tokens=450):\n",
        "    \"\"\"Split long text into chunks based on token length.\"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
        "\n",
        "def chunk_text_with_stride(text, max_length=384, stride=128):\n",
        "    inputs = tokenizer(text, return_overflowing_tokens=True, max_length=max_length, stride=stride, truncation=True, padding=\"max_length\")\n",
        "    return [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in inputs[\"input_ids\"]]\n",
        "\n",
        "\n",
        "def rank_chunks_semantic(question, chunks, top_k=3):\n",
        "    question_emb = embedder.encode(question, convert_to_tensor=True)\n",
        "    chunk_embs = embedder.encode(chunks, convert_to_tensor=True)\n",
        "    scores = util.cos_sim(question_emb, chunk_embs)[0]\n",
        "    top_indices = scores.argsort(descending=True)[:top_k]\n",
        "    return [chunks[i] for i in top_indices]\n",
        "\n",
        "def rank_chunks_tfidf(question, chunks, top_k=3):\n",
        "    \"\"\"Rank text chunks based on TF-IDF similarity to the question.\"\"\"\n",
        "    vectorizer = TfidfVectorizer().fit([question] + chunks)\n",
        "    vectors = vectorizer.transform([question] + chunks)\n",
        "    scores = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
        "    top_indices = scores.argsort()[-top_k:][::-1]\n",
        "    return [chunks[i] for i in top_indices]\n",
        "\n",
        "def is_valid_answer(answer_text, score, min_len=3, min_score=0.1):\n",
        "    \"\"\"Check if the answer is valid based on length and confidence.\"\"\"\n",
        "    return len(answer_text.strip()) >= min_len and score >= min_score\n",
        "\n",
        "def get_best_answer(question, text_chunks):\n",
        "    \"\"\"Run QA model on ranked chunks and return best answer.\"\"\"\n",
        "    answers = []\n",
        "    for chunk in text_chunks:\n",
        "        try:\n",
        "            result = qa_pipeline({'question': question, 'context': chunk})\n",
        "            if is_valid_answer(result['answer'], result['score']):\n",
        "                cleaned = re.sub(r'[^\\w\\s,.]', '', result['answer']).strip()\n",
        "                answers.append((cleaned, result['score']))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"QA failed on chunk: {e}\")\n",
        "    return max(answers, key=lambda x: x[1])[0] if answers else \"No good answer found.\"\n",
        "\n",
        "def answer_question(question, full_text):\n",
        "    \"\"\"Top-level QA function: cleans, expands, chunks, ranks, answers.\"\"\"\n",
        "    question_expanded = expand_question(question.lower())\n",
        "    cleaned_text = clean_text(full_text)\n",
        "    chunks = chunk_text_with_stride(cleaned_text)\n",
        "    top_chunks = rank_chunks_semantic(question_expanded, chunks)\n",
        "    return get_best_answer(question_expanded, top_chunks)\n",
        "\n",
        "def is_short_answer(example, max_words=3):\n",
        "    # Use the first answer in case there are multiple\n",
        "    answer = example[\"answers\"][\"text\"][0]\n",
        "    return len(answer.strip().split()) <= max_words\n",
        "\n",
        "\n",
        "def developPipelines():\n",
        "    qa_pipelines = []\n",
        "    for count in range(0, 21):\n",
        "        #path = f\"/content/drive/My Drive/Distil-BERT/QA/Model{count}\"\n",
        "        path = f\"/content/drive/My Drive/Colab Notebooks/Distil-BERT/QA/Model{count}\"\n",
        "        qa = pipeline(\"question-answering\", model=path, tokenizer=path)\n",
        "        qa_pipelines.append(qa)\n",
        "    return qa_pipelines\n",
        "\n",
        "qa_pipelines = developPipelines()\n"
      ],
      "metadata": {
        "id": "VdyJTmGKvaD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = []\n",
        "# Load the validation set\n",
        "dataset = load_dataset(\"squad\", split=\"validation\")\n",
        "dataset = dataset.filter(is_short_answer)\n",
        "dataset = dataset.shuffle(seed=42).select(range(100))\n",
        "for count in range(0, 20):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    kw_model = KeyBERT()\n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    qa_pipeline = qa_pipelines[count]\n",
        "    #tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Distil-BERT/QA/Model1\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Colab Notebooks/Distil-BERT/QA/Model1\")\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    for item in tqdm(dataset):\n",
        "        question = item[\"question\"]\n",
        "        context = item[\"context\"]\n",
        "        ground_truths = item[\"answers\"][\"text\"]\n",
        "\n",
        "        #Run model\n",
        "        predicted_answer = answer_question(question, context)\n",
        "\n",
        "        #Save predictions and references\n",
        "        predictions.append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"prediction_text\": predicted_answer\n",
        "        })\n",
        "\n",
        "        references.append({\n",
        "            \"id\": item[\"id\"],\n",
        "            \"answers\": {\n",
        "                \"answer_start\": item[\"answers\"][\"answer_start\"],\n",
        "                \"text\": ground_truths\n",
        "            }\n",
        "        })\n",
        "    squad_metric = evaluate.load(\"squad\")\n",
        "\n",
        "    results = squad_metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    #Create binary labels: 1 if prediction has F1 > 0, else 0\n",
        "    binary_preds = []\n",
        "    binary_refs = []\n",
        "\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        pred_text = pred[\"prediction_text\"]\n",
        "        gold_texts = ref[\"answers\"][\"text\"]\n",
        "\n",
        "        #Match if exact or partial overlap\n",
        "        matched = any(pred_text.strip().lower() in gt.lower() or gt.lower() in pred_text.strip().lower() for gt in gold_texts)\n",
        "\n",
        "        binary_preds.append(1 if matched else 0)\n",
        "        binary_refs.append(1)  #All references are positives\n",
        "\n",
        "    #Accuracy: how many were correct\n",
        "    acc = accuracy_score(binary_refs, binary_preds)\n",
        "    list1 = [results['exact_match'], results['f1'], acc]\n",
        "    metrics.append(list1)\n"
      ],
      "metadata": {
        "id": "PFhp-elzvguj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0\n",
        "for row in metrics:\n",
        "    i += 1\n",
        "    print(f\"Evaluation Results : Model {i}:\")\n",
        "    print(f\"Exact Match (EM): {row[0]:.2f}\")\n",
        "    print(f\"F1 Score: {row[1]:.2f}\")\n",
        "    print(f\"Accuracy: {row[2]:.2f}\")"
      ],
      "metadata": {
        "id": "_E_2bOYGvlgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose metrics\n",
        "em_scores = [row[0] for row in metrics]\n",
        "f1_scores = [row[1] for row in metrics]\n",
        "accuracy_scores = [row[2] * 100 for row in metrics]\n",
        "\n",
        "# Numeric x-axis\n",
        "x = list(range(len(metrics)))\n",
        "model_labels = [f\"Model {i+1}\" for i in x]\n",
        "\n",
        "# Compute average for each model\n",
        "average_scores = [(em + f1 + acc) / 3 for em, f1, acc in zip(em_scores, f1_scores, accuracy_scores)]\n",
        "best_model_index = average_scores.index(max(average_scores))\n",
        "best_model_label = model_labels[best_model_index]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x, em_scores, label=\"Exact Match (EM)\", marker='o')\n",
        "plt.plot(x, f1_scores, label=\"F1 Score\", marker='o')\n",
        "plt.plot(x, accuracy_scores, label=\"Accuracy\", marker='o')\n",
        "\n",
        "# Annotate max values\n",
        "def annotate_max(values, label):\n",
        "    idx = values.index(max(values))\n",
        "    plt.annotate(f\"Max {label}: {max(values):.2f}\\n({model_labels[idx]})\",\n",
        "                 (x[idx], values[idx]),\n",
        "                 textcoords=\"offset points\",\n",
        "                 xytext=(0,10), ha='center',\n",
        "                 fontsize=9, color='red', fontweight='bold')\n",
        "\n",
        "annotate_max(em_scores, \"EM\")\n",
        "annotate_max(f1_scores, \"F1\")\n",
        "annotate_max(accuracy_scores, \"Accuracy\")\n",
        "\n",
        "# Highlight best overall model\n",
        "plt.axvline(x=best_model_index, color='gray', linestyle='--', alpha=0.5)\n",
        "plt.text(best_model_index, 95, f\"Best Avg: {best_model_label}\\n({average_scores[best_model_index]:.2f})\",\n",
        "         ha='center', fontsize=9, bbox=dict(facecolor='yellow', alpha=0.3))\n",
        "\n",
        "# Finalize plot\n",
        "plt.xticks(ticks=x, labels=model_labels, rotation=45)\n",
        "plt.title(\"Evaluation Metrics per Model\")\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0, 100)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ttDJRcWHvpss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}