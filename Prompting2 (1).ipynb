{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXKHrdc2v2qT",
        "outputId": "719d6668-499a-4753-fd91-3ac34309d509"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from keybert) (2.0.2)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.11/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.11/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.6.15)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#PIP INSTALLS:\n",
        "!pip install wikipedia keybert sentence-transformers transformers spacy datasets evaluate matplotlib accelerate nltk torch\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m nltk.downloader averaged_perceptron_tagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jg5LDs5rwCu4",
        "outputId": "ea4be6ca-d293-40c2-bbd7-74099bfd209c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "#IMPORTS:\n",
        "import nltk\n",
        "from itertools import product\n",
        "import wikipedia\n",
        "from sentence_transformers import SentenceTransformer, util, InputExample, losses, models\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "import re\n",
        "import logging\n",
        "from transformers import pipeline\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "from google.colab import drive, files\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "drive.mount(\"/content/drive\")\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow0HeH_hwGr2",
        "outputId": "4e03027a-9b67-4ba5-f633-c9f5e8634c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config.json\t\t\t   README.md\t\t      tokenizer.json\n",
            "config_sentence_transformers.json  sentence_bert_config.json  vocab.txt\n",
            "model.safetensors\t\t   special_tokens_map.json\n",
            "modules.json\t\t\t   tokenizer_config.json\n"
          ]
        }
      ],
      "source": [
        "# 3. If 'QA' is inside 'Distil-BERT', run this\n",
        "!ls \"/content/drive/My Drive/Distil-BERT/1/QA/SentenceTransformer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-AVPdGtjwSr0"
      },
      "outputs": [],
      "source": [
        "#DEFINITIONS:\n",
        "EXCLUDE_TAGS = {\"IN\", \"CC\"}\n",
        "# Common slang dictionary\n",
        "SLANG_DICT = {\n",
        "    \"u\": \"you\", \"ur\": \"your\", \"r\": \"are\", \"y\": \"why\",\n",
        "    \"wat\": \"what\", \"wats\": \"what is\", \"wanna\": \"want to\",\n",
        "    \"gonna\": \"going to\", \"tho\": \"though\", \"pls\": \"please\",\n",
        "    \"thx\": \"thanks\", \"lol\": \"\", \"omg\": \"oh my god\",\n",
        "    \"idk\": \"I don't know\", \"btw\": \"by the way\", \"smh\": \"shaking my head\",\n",
        "    \"bff\": \"best friend forever\", \"brb\": \"be right back\", \"tbh\": \"to be honest\",\n",
        "    \"fyi\": \"for your information\", \"lmao\": \"laughing my ass off\", \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"ttyl\": \"talk to you later\", \"np\": \"no problem\", \"yolo\": \"you only live once\",\n",
        "    \"fam\": \"family\", \"lit\": \"exciting or excellent\", \"savage\": \"ruthless or harsh\",\n",
        "    \"slay\": \"to do something exceptionally well\", \"bae\": \"before anyone else\", \"cray\": \"crazy\",\n",
        "    \"fomo\": \"fear of missing out\", \"swag\": \"style or coolness\", \"vibe\": \"mood or atmosphere\",\n",
        "    \"tbh\": \"to be honest\", \"imo\": \"in my opinion\", \"srsly\": \"seriously\", \"jk\": \"just kidding\",\n",
        "    \"ppl\": \"people\", \"wyd\": \"what you doing\", \"smexy\": \"sexy\", \"af\": \"as f*ck\",\n",
        "    \"wth\": \"what the hell\", \"wtf\": \"what the f*ck\", \"asl\": \"age, sex, location\",\n",
        "    \"ik\": \"I know\", \"ikr\": \"I know, right?\", \"lml\": \"laughing madly\", \"hmu\": \"hit me up\",\n",
        "    \"mfw\": \"my face when\", \"fml\": \"f*ck my life\", \"yass\": \"yes\"\n",
        "}\n",
        "\n",
        "def is_permutable(pos_tag):\n",
        "    return pos_tag not in EXCLUDE_TAGS\n",
        "\n",
        "def generate_cased_variants(words, tags):\n",
        "    options = []\n",
        "\n",
        "    for word, tag in zip(words, tags):\n",
        "        if word and word[0].isalpha() and is_permutable(tag):\n",
        "            # Only change the first character\n",
        "            lower = word[0].lower() + word[1:]\n",
        "            upper = word[0].upper() + word[1:]\n",
        "            options.append([lower, upper])\n",
        "        else:\n",
        "            options.append([word])\n",
        "\n",
        "    # Generate all combinations\n",
        "    return [' '.join(p) for p in product(*options)]\n",
        "\n",
        "# Test function\n",
        "def case_variants(text):\n",
        "    words = text.split()\n",
        "    tags = [tag for _, tag in nltk.pos_tag(words)]\n",
        "    return generate_cased_variants(words, tags)\n",
        "\n",
        "def extract_keyword_spacy_keybert(question):\n",
        "    noun_phrases = []\n",
        "    for question in case_variants(question):\n",
        "        doc = nlp(question)\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        priority_entity_labels = [\"PERSON\", \"ORG\", \"GPE\", \"FAC\", \"LOC\"]\n",
        "        named_entities = [ent.text for ent in doc.ents if ent.label_ in priority_entity_labels]\n",
        "\n",
        "        if named_entities:\n",
        "            return named_entities[0]\n",
        "\n",
        "        noun_phrases += [chunk.text for chunk in doc.noun_chunks]\n",
        "\n",
        "    keywords = kw_model.extract_keywords(question.title(), top_n=3, stop_words='english')\n",
        "    bert_keywords = [kw[0] for kw in keywords]\n",
        "\n",
        "\n",
        "    candidates = list(dict.fromkeys(noun_phrases + bert_keywords))\n",
        "\n",
        "    filtered_candidates = [\n",
        "        c for c in candidates\n",
        "        if c.lower() not in STOP_WORDS and len(c.strip()) > 2]\n",
        "\n",
        "\n",
        "    filtered_candidates = sorted(filtered_candidates, key=lambda x: question_lower.find(x.lower()))\n",
        "\n",
        "    return filtered_candidates[0] if filtered_candidates else question\n",
        "\n",
        "#alternative technique\n",
        "def get_wikipedia_article(query):\n",
        "    try:\n",
        "\n",
        "        try:\n",
        "            page = wikipedia.page(query, auto_suggest=False)\n",
        "            print(f\"Retrieved (exact match): {page.title}\")\n",
        "            return page.content\n",
        "        except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
        "            pass\n",
        "\n",
        "\n",
        "        search_results = wikipedia.search(query)\n",
        "        if not search_results:\n",
        "            print(\"No search results found.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        for title in search_results:\n",
        "            try:\n",
        "                page = wikipedia.page(title)\n",
        "                if \"replica\" not in page.title.lower():\n",
        "                    print(f\"Retrieved: {page.title}\")\n",
        "                    return page.content\n",
        "            except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
        "                continue\n",
        "\n",
        "        print(\"No valid Wikipedia page found.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_best_wikipedia_article(question, top_n=5):\n",
        "    # Step 1: Search using the full question\n",
        "    search_results = wikipedia.search(question, results=top_n)\n",
        "\n",
        "\n",
        "    print(\"üîé Candidates:\", search_results)\n",
        "    # Step 2: Get summaries of each\n",
        "    summaries = []\n",
        "    titles = []\n",
        "    for title in search_results:\n",
        "        try:\n",
        "            summary = wikipedia.summary(title, sentences=3)\n",
        "            summaries.append(summary)\n",
        "            titles.append(title)\n",
        "        except wikipedia.exceptions.DisambiguationError:\n",
        "            continue\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            continue\n",
        "\n",
        "    if not summaries:\n",
        "        return None\n",
        "    case_vars = case_variants(question)\n",
        "    summary_embeddings = embedder.encode(summaries, convert_to_tensor=True)\n",
        "    cosine_scores = []\n",
        "    for question in case_vars:\n",
        "        # Step 3: Use SentenceTransformer to rank similarity\n",
        "        question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "        cosine_scores.append(util.cos_sim(question_embedding, summary_embeddings))\n",
        "    score_matrix = torch.vstack(cosine_scores)  # shape: [V x N]\n",
        "    best_scores, _ = score_matrix.max(dim=0)\n",
        "    best_idx = best_scores.argmax().item()\n",
        "    best_title = titles[best_idx]\n",
        "    best_page = wikipedia.page(best_title)\n",
        "\n",
        "    print(f\"‚úÖ Best match: {best_title}\")\n",
        "    return best_page.content, best_title\n",
        "\n",
        "def expand_question(question):\n",
        "    \"\"\"Replace common slangs and shorthand in questions.\"\"\"\n",
        "    words = question.lower().split()\n",
        "    expanded = [SLANG_DICT.get(w, w) for w in words]\n",
        "    return ' '.join(expanded)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove unwanted characters and normalize spacing.\"\"\"\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove references like [1]\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "#alternative technique\n",
        "def chunk_text_by_tokens(text, max_tokens=450):\n",
        "    \"\"\"Split long text into chunks based on token length.\"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
        "\n",
        "def chunk_text_with_stride(text, max_length=384, stride=128):\n",
        "    inputs = tokenizer(text, return_overflowing_tokens=True, max_length=max_length, stride=stride, truncation=True, padding=\"max_length\")\n",
        "    return [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in inputs[\"input_ids\"]]\n",
        "\n",
        "\n",
        "def rank_chunks_semantic(question, chunks, top_k=3):\n",
        "    question_emb = embedder.encode(question, convert_to_tensor=True)\n",
        "    chunk_embs = embedder.encode(chunks, convert_to_tensor=True)\n",
        "    scores = util.cos_sim(question_emb, chunk_embs)[0]\n",
        "    top_indices = scores.argsort(descending=True)[:top_k]\n",
        "    return [chunks[i] for i in top_indices]\n",
        "\n",
        "#alternative technique\n",
        "def rank_chunks_tfidf(question, chunks, top_k=3):\n",
        "    \"\"\"Rank text chunks based on TF-IDF similarity to the question.\"\"\"\n",
        "    vectorizer = TfidfVectorizer().fit([question] + chunks)\n",
        "    vectors = vectorizer.transform([question] + chunks)\n",
        "    scores = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
        "    top_indices = scores.argsort()[-top_k:][::-1]\n",
        "    return [chunks[i] for i in top_indices]\n",
        "\n",
        "def is_valid_answer(answer_text, score, min_len=3, min_score=0.1):\n",
        "    \"\"\"Check if the answer is valid based on length and confidence.\"\"\"\n",
        "    return len(answer_text.strip()) >= min_len and score >= min_score\n",
        "\n",
        "def get_best_answer(question, text_chunks):\n",
        "    \"\"\"Run QA model on ranked chunks and return best answer.\"\"\"\n",
        "    answers = []\n",
        "    for chunk in text_chunks:\n",
        "        try:\n",
        "            result = qa_pipeline({'question': question, 'context': chunk})\n",
        "            if is_valid_answer(result['answer'], result['score']):\n",
        "                cleaned = re.sub(r'[^\\w\\s,.]', '', result['answer']).strip()\n",
        "                answers.append((cleaned, result['score']))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"QA failed on chunk: {e}\")\n",
        "    return max(answers, key=lambda x: x[1])[0] if answers else \"No good answer found.\"\n",
        "\n",
        "def answer_question(question, full_text):\n",
        "    \"\"\"Top-level QA function: cleans, expands, chunks, ranks, answers.\"\"\"\n",
        "    question_expanded = expand_question(question.lower())\n",
        "    cleaned_text = clean_text(full_text)\n",
        "    chunks = chunk_text_with_stride(cleaned_text)\n",
        "    top_chunks = rank_chunks_semantic(question_expanded, chunks)\n",
        "    return get_best_answer(question_expanded, top_chunks)\n",
        "\n",
        "def is_short_answer(example, max_words=3):\n",
        "    # Use the first answer in case there are multiple\n",
        "    answer = example[\"answers\"][\"text\"][0]\n",
        "    return len(answer.strip().split()) <= max_words\n",
        "\n",
        "def prepare_train_features(examples):\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=384,\n",
        "        stride=128,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                start_positions.append(cls_index)\n",
        "                end_positions.append(cls_index)\n",
        "            else:\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                start_positions.append(token_start_index - 1)\n",
        "\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                end_positions.append(token_end_index + 1)\n",
        "\n",
        "    tokenized_examples[\"start_positions\"] = start_positions\n",
        "    tokenized_examples[\"end_positions\"] = end_positions\n",
        "    return tokenized_examples\n",
        "\n",
        "def trainModel(model, tokenizer, dataset):\n",
        "    print(\"training\")\n",
        "    # Optimizer\n",
        "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    batch_num = 0\n",
        "    totalLoss = 0;\n",
        "    LossVector = []\n",
        "    for epoch in range(3):\n",
        "        small_train = dataset\n",
        "\n",
        "    # Tokenize the reduced dataset\n",
        "        tokenized_squad = small_train.map(\n",
        "            prepare_train_features,\n",
        "            batched=True,\n",
        "            remove_columns=small_train.column_names\n",
        "        )\n",
        "\n",
        "\n",
        "    # DataLoader\n",
        "        train_loader = DataLoader(tokenized_squad, batch_size=16, shuffle=True, collate_fn=default_data_collator)\n",
        "        progress = tqdm(train_loader)\n",
        "        for batch in progress:\n",
        "            batch_num += 1\n",
        "\n",
        "            input_ids = batch[\"input_ids\"]\n",
        "            attention_mask = batch[\"attention_mask\"]\n",
        "            start_positions = batch[\"start_positions\"]\n",
        "            end_positions = batch[\"end_positions\"]\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                start_positions=start_positions,\n",
        "                end_positions=end_positions,\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            totalLoss += loss.item()\n",
        "            LossVector.append(loss.item())\n",
        "            avg = totalLoss / batch_num\n",
        "\n",
        "            progress.set_description(f\"Loss: {avg:.4f}\")\n",
        "    save_path = f\"/content/drive/My Drive/Colab Notebooks/Distil-BERT/QA/Model6\"\n",
        "    model.save_pretrained(save_path)\n",
        "    tokenizer.save_pretrained(save_path)\n",
        "    print(f\"Model saved at {save_path}\")\n",
        "    print(f\"Model 6 Loss Vector: {LossVector}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7cMwIB8wZp9",
        "outputId": "22f1ae2b-0761-46be-fe52-fbf672e2602e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence‚Äëtransformers version: 5.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q sentence-transformers   # upgrades to 3.x as of July‚ÄØ2025\n",
        "import sentence_transformers, os, torch\n",
        "print(\"sentence‚Äëtransformers version:\", sentence_transformers.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_CiNVuPYpoA",
        "outputId": "b1789acb-1818-4c47-f5b5-a3cbe487121a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 88M\n",
            "-rw------- 1 root root  617 Dec 31  1979 config.json\n",
            "-rw------- 1 root root  205 Dec 31  1979 config_sentence_transformers.json\n",
            "-rw------- 1 root root  87M Dec 31  1979 model.safetensors\n",
            "-rw------- 1 root root  349 Dec 31  1979 modules.json\n",
            "-rw------- 1 root root 9.5K Dec 31  1979 README.md\n",
            "-rw------- 1 root root   53 Dec 31  1979 sentence_bert_config.json\n",
            "-rw------- 1 root root  695 Dec 31  1979 special_tokens_map.json\n",
            "-rw------- 1 root root 1.5K Dec 31  1979 tokenizer_config.json\n",
            "-rw------- 1 root root 695K Dec 31  1979 tokenizer.json\n",
            "-rw------- 1 root root 227K Dec 31  1979 vocab.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -lh \"/content/drive/MyDrive/Distil-BERT/1/QA/SentenceTransformer\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keHAQYzOYmqu"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grgRYJ7XXjH9",
        "outputId": "abb9caac-428e-4c43-b3e5-41ea36cfb46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers huggingface-hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYF-LPo7whE_",
        "outputId": "37dd7b5b-06fa-4ce1-b3b2-fcf4bf33b1df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: How many continents are there?\n",
            "Q: How many continents are there?\n",
            "üîé Candidates: ['Continent', 'Americas', 'Europe', 'Asia', 'Chronology of continents']\n",
            "‚úÖ Best match: Chronology of continents\n",
            "A: seven\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import models, SentenceTransformer\n",
        "from keybert                import KeyBERT\n",
        "from transformers           import DistilBertTokenizerFast\n",
        "\n",
        "ROOT = \"/content/drive/MyDrive/Distil-BERT/1/QA\"   # ‚Üê no space in ‚ÄúMyDrive‚Äù\n",
        "\n",
        "# ‚îÄ‚îÄ 1. Create an ST model by hand ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "#     ‚Ä¢ word-embedding module  (your fine-tuned DistilBERT)\n",
        "#     ‚Ä¢ pooling module         (add mean-pooling, dimension auto-detected)\n",
        "\n",
        "bert   = models.Transformer(f\"{ROOT}/SentenceTransformer\", max_seq_length=512)\n",
        "pool   = models.Pooling(\n",
        "           word_embedding_dimension = bert.get_word_embedding_dimension(),  # 768 for DistilBERT\n",
        "           pooling_mode_mean_tokens = True,\n",
        "           pooling_mode_cls_token  = False,\n",
        "           pooling_mode_max_tokens = False\n",
        "         )\n",
        "st_embedder = SentenceTransformer(modules=[bert, pool])\n",
        "\n",
        "# ‚îÄ‚îÄ 2. Feed that into KeyBERT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "kw_model = KeyBERT(model=st_embedder)\n",
        "\n",
        "# ‚îÄ‚îÄ 3. Everything else stays the same ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "embedder  = SentenceTransformer('all-MiniLM-L6-v2')        # keep if you still need it\n",
        "path      = f\"{ROOT}/Model5\"\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
        "              f\"{ROOT}/Model1\",\n",
        "              local_files_only=True\n",
        "           )\n",
        "\n",
        "\n",
        "while True:\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=path, tokenizer=path)\n",
        "    question = input(\"Enter your question: \")\n",
        "    if question.lower() == 'break':\n",
        "        break\n",
        "    else:\n",
        "        if question.lower() == 'wrong answer':\n",
        "            dataset = []\n",
        "            title = ''\n",
        "            if input(\"Was the context incorrect?\").lower()[0] == 'y':\n",
        "                if input(\"Provide correct context?\").lower()[0] == 'y':\n",
        "                    if input(\"Attach textfile with correct context?\").lower()[0] == 'y':\n",
        "                        uploaded = files.upload()\n",
        "                        for fn in uploaded.keys():\n",
        "                            print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "                                name=fn, length=len(uploaded[fn])))\n",
        "                            with open(fn, 'r') as file:\n",
        "                                dataset += file.readlines()\n",
        "                    else:\n",
        "                        dataset += input(\"CONTEXT: \")\n",
        "                    for line in dataset:\n",
        "                        article_text += line\n",
        "            else:\n",
        "                dataset.append(article_text)\n",
        "\n",
        "            if input(\"Was the title incorrect?\").lower()[0] == 'y':\n",
        "                if input(\"Provide correct title?\").lower()[0] == 'y':\n",
        "                    title = input(\"TITLE: \")\n",
        "            else:\n",
        "                title = best_title\n",
        "            answer = input(\"ANSWER: \")\n",
        "            if not (title == '' or dataset == []):\n",
        "                new_example = {\n",
        "                    \"id\": \"user_example_001\",\n",
        "                    \"title\": title,\n",
        "                    \"context\": article_text,\n",
        "                    \"question\": question,\n",
        "                    \"answers\": {\n",
        "                        \"text\": [answer],\n",
        "                        \"answer_start\": [article_text.find(answer)]\n",
        "                    }\n",
        "                }\n",
        "                dataset = Dataset.from_dict({key: [value] for key, value in new_example.items()})\n",
        "\n",
        "                train_examples = [\n",
        "                    InputExample(texts=[question, title])\n",
        "                ]\n",
        "\n",
        "                # DataLoader\n",
        "                train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "\n",
        "                # Define loss\n",
        "                train_loss = losses.MultipleNegativesRankingLoss(kw_model.model.embedding_model)\n",
        "\n",
        "                # Train the model\n",
        "                import wandb\n",
        "                from unittest.mock import patch\n",
        "                with patch(\"builtins.input\", return_value=\"1\"):\n",
        "                    wandb.login(anonymous=\"allow\")\n",
        "\n",
        "                kw_model.model.embedding_model.fit(\n",
        "                    train_objectives=[(train_dataloader, train_loss)],\n",
        "                    epochs=5,\n",
        "                    warmup_steps=100,\n",
        "                    output_path=f\"/content/drive/My Drive/Colab Notebooks/Distil-BERT/QA/SentenceTransformer\",\n",
        "\n",
        "                )\n",
        "\n",
        "                # Save\n",
        "                kw_model.model.embedding_model.save(f\"/content/drive/My Drive/Colab Notebooks/Distil-BERT/QA/SentenceTransformer\")\n",
        "                trainModel(qa_pipeline.model, tokenizer, dataset)\n",
        "    print(\"Q:\", question)\n",
        "    article_text, best_title = get_best_wikipedia_article(extract_keyword_spacy_keybert(question))\n",
        "    print(\"A:\", answer_question(question, article_text))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}